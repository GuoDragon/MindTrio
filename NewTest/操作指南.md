# 中文篇章级句间关系分析任务操作指南

## 1. 环境准备

### 1.1 华为云ModelArts环境配置
#### 1.1.1 创建Notebook实例
1. 注册并登录华为云：访问https://www.huaweicloud.com/，完成注册并登录账号。
2. 进入AI开发平台ModelArts：在控制台中找到"人工智能"分类下的"AI开发平台ModelArts"，点击"进入控制台"。
3. 创建Notebook实例：
   - 区域选择：贵阳一
   - 镜像选择：`mindspore_2_7-vllm-mindspeed-cann8_2alpha2_ubuntu22`（名称）、`ma20250630`（版本）、`mindspore0904`（所属组织）
   - 实例规格：`Ascend: 1*ascend-snt9b1|ARM: 24核 192GB`
   - 磁盘规格：50GB
   - 点击"创建"完成实例创建。

#### 1.1.2 配置环境
1. 打开Terminal：在Notebook实例中点击"Terminal"按钮打开终端。
2. 安装依赖：
   ```bash
   pip install -r requirements.txt
   ```
3. 配置Jupyter Notebook内核：
   ```bash
   pip install ipykernel
   python -m ipykernel install --prefix=/home/ma-user/.local --name=py310 --display-name "Python 3.10"
   ```
4. 刷新界面：刷新Notebook Launcher界面，选择"Python 3.10"内核新建Notebook文件。

### 1.2 代码上传
将`D:\hw\NewTest`目录下的所有文件上传到华为云ModelArts的工作目录（通常为`/home/ma-user/work/`）。

## 2. 数据准备
将训练数据`train.json`和验证数据`val.json`上传到华为云ModelArts的`/home/ma-user/work/data/`目录下。

数据格式要求：
```json
{
    "content": "输入文本内容",
    "summary": "关系分类结果及解释"
}
```

## 3. 模型训练

### 3.1 运行训练脚本
1. 在Jupyter Notebook中打开`train.ipynb`文件。
2. 按顺序执行所有单元格。
3. 训练过程中会生成检查点文件，默认保存路径为`./output/`。

### 3.2 训练参数说明
- `per_device_train_batch_size`: 4
- `gradient_accumulation_steps`: 5
- `logging_steps`: 10
- `num_train_epochs`: 3
- `save_steps`: 100
- `learning_rate`: 3e-5

## 4. 模型合并

### 4.1 运行合并脚本
1. 在Jupyter Notebook中打开`merge.ipynb`文件。
2. 修改`lora_path`为实际的训练检查点路径（例如：`./output/checkpoint-1380`）。
3. 按顺序执行所有单元格。
4. 合并后的模型会保存到`merged_path`指定的目录（默认：`./merged_qwen_lora/`）。

## 5. 模型推理

### 5.1 使用合并后的模型进行推理
1. 在`merge.ipynb`文件的最后一个单元格中，修改`test_samples`为你要测试的文本。
2. 执行该单元格，即可得到模型的推理结果。

### 5.2 推理结果示例
```
输入: 月亮又圆又亮，所以古人称之为玉盘。
输出: 因果
原因：前半句话描述了月亮的特征“又圆又亮”，后半句话说明古人因为这个特征所以称之为“玉盘”，两者之间存在因果关系。
```

## 6. 旧模型权重迁移
如果需要使用旧实验的最佳权重（`D:\hw\checkpoint-1380`），请将该目录上传到华为云ModelArts的工作目录，并在`merge.ipynb`中修改`lora_path`为该目录的路径。

## 7. 版本兼容性注意事项 ⚠️

### 7.1 关键版本要求
- **mindnlp版本**：必须使用 `mindnlp==0.5.1`
- **mindspore版本**：`mindspore==2.7.0`
- **transformers版本**：推荐 `4.40-4.45` 范围内的版本（避免使用4.56+）

### 7.2 训练参数配置的重要限制

#### ❌ 不要使用的参数
在 `TrainingArguments` 中**绝对不要**添加以下参数：

1. **`fp16=True`**
   - 问题：会导致 `AttributeError: module 'mindtorch.npu' has no attribute 'amp'` 错误
   - 原因：mindnlp 0.5.1 与 MindSpore NPU 的混合精度训练存在兼容性问题
   - 解决：使用 `ms_dtype=mindspore.bfloat16` 代替

2. **`gradient_checkpointing=True`**
   - 问题：可能导致额外的兼容性问题
   - 解决：不使用梯度检查点，保持默认配置

#### ✅ 推荐的数据类型配置
```python
# 在加载模型时使用 bfloat16
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    ms_dtype=mindspore.bfloat16,  # 推荐：在 Ascend NPU 上稳定
    device_map=0
)
```

### 7.3 旧版本迁移说明
如果您之前使用旧版本 mindnlp 成功训练过模型：
- ✅ 旧版本的 checkpoint（如 `checkpoint-1380`）可以直接使用
- ✅ 可以使用新脚本进行模型合并和推理
- ⚠️ 但重新训练时必须遵循新版本的参数限制

### 7.4 其他注意事项
1. **资源限制**：训练过程需要较多的内存和NPU资源，请确保实例规格满足要求（推荐：Ascend: 1*ascend-snt9b1 | ARM: 24核 192GB）。
2. **数据量**：建议至少有1000条以上的训练数据，以获得较好的模型效果。
3. **推理速度**：合并后的模型推理速度会比使用LoRA时稍慢，但精度相同。
4. **依赖冲突**：如果遇到依赖版本冲突，请严格按照 `requirements.txt` 中的版本安装。

## 8. 常见问题

### 8.1 训练过程中出现OOM错误
- 解决方案：减小`per_device_train_batch_size`或增加`gradient_accumulation_steps`。

### 8.2 模型加载失败
- 解决方案：检查模型路径是否正确，确保所有模型文件都已上传到指定目录。

### 8.3 推理结果不符合预期
- 解决方案：增加训练数据量、调整模型参数或优化数据标注质量。

### 8.4 出现 `mindtorch.npu.amp` 错误
- **错误信息**：`AttributeError: module 'mindtorch.npu' has no attribute 'amp'`
- **原因**：在 `TrainingArguments` 中使用了 `fp16=True` 参数
- **解决方案**：
  1. 删除 `fp16=True` 参数
  2. 使用 `ms_dtype=mindspore.bfloat16` 代替（在模型加载时）
  3. 参考本目录下的 `train.ipynb` 文件中的正确配置

### 8.5 transformers 版本冲突
- **问题**：安装了 `transformers==4.56.2` 导致 API 不兼容
- **解决方案**：降级到兼容版本
  ```bash
  pip uninstall transformers
  pip install transformers==4.45.0
  ```

### 8.6 数据文件找不到
- **错误信息**：`FileNotFoundError: train.json not found`
- **解决方案**：
  1. 确认数据文件已上传到 `/home/ma-user/work/data/` 目录
  2. 检查文件名是否正确（区分大小写）
  3. 使用 `ls /home/ma-user/work/data/` 命令查看文件列表
