{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# 中文篇章级句间关系分类 - LoRA微调实验\n",
    "\n",
    "## 实验说明\n",
    "\n",
    "本实验使用 **MindSpore + mindnlp 0.5.1 + LoRA** 在 **DeepSeek-R1-Distill-Qwen-1.5B** 模型上进行微调。\n",
    "\n",
    "### 任务目标\n",
    "- **输入**：一个句子或对话片段\n",
    "- **输出**：该句子所属的PDTB篇章关系分类（扩展/因果/比较/并列/其他）以及分类原因\n",
    "\n",
    "### 版本信息\n",
    "- mindnlp: 0.5.1\n",
    "- mindspore: 2.7.0\n",
    "- transformers: ~4.40-4.45（推荐）\n",
    "- 数据类型: bfloat16（在Ascend NPU上更稳定）\n",
    "\n",
    "### ⚠️ 重要注意事项\n",
    "1. **不要使用 `fp16=True`**：会导致 `mindtorch.npu.amp` 错误\n",
    "2. **不要使用 `gradient_checkpointing=True`**：可能导致兼容性问题\n",
    "3. **使用 `bfloat16` 数据类型**：在Ascend NPU上表现更稳定\n",
    "4. **数据路径**：确保数据已上传至 `/home/ma-user/work/data/`\n",
    "\n",
    "### 训练环境\n",
    "- 镜像：ms2.7.1-cann8.2rc1（名称）、v3（版本）、mindspore courses（所属组织）\n",
    "- 实例规格：Ascend: 1*ascend-snt9b1 | ARM: 24核 192GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-cell-header",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/work/lcl/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/work/lcl/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/ma-user/work/lcl/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ma-user/work/lcl/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/ma-user/work/lcl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Modular Diffusers is currently an experimental feature under active development. The API is subject to breaking changes in future releases.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mindnlp版本: 0.5.0rc2\n",
      "mindspore版本: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "# 核心框架\n",
    "import mindnlp\n",
    "import mindspore\n",
    "\n",
    "# 数据处理\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 模型和训练相关\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# LoRA相关\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# 查看版本信息\n",
    "print(f\"mindnlp版本: {mindnlp.__version__}\")\n",
    "print(f\"mindspore版本: {mindspore.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-load-header",
   "metadata": {},
   "source": [
    "## 2. 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data-load-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数: 9198\n",
      "验证集样本数: 1500\n",
      "\n",
      "数据集前3个样本:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'content': ['他的有没有什么不足之处？我觉得他可以就是加一些他自己的感受，因为他如果光只说那些一系列的动作，就感觉很空白，没有什么情感在里面。',\n",
       "  '星汉是什么？银河。',\n",
       "  '对于花来说没有人欣赏是多么的悲惨，就像我们姑娘把自己打扮得花枝招展，却没有人欣赏一样是一种不幸'],\n",
       " 'summary': ['扩展\\n原因：前半句话提出问题，询问他的不足之处，后半句话则具体回答了我认为的他的不足之处，所以属于扩展关系。',\n",
       "  '扩展\\n原因：',\n",
       "  '扩展\\n原因：']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据路径（华为云ModelArts路径）\n",
    "train_path = \"/home/ma-user/work/data/train.json\"\n",
    "val_path = \"/home/ma-user/work/data/val.json\"\n",
    "\n",
    "# 读取数据\n",
    "df_train = pd.read_json(train_path)\n",
    "df_val = pd.read_json(val_path)\n",
    "\n",
    "# 转换为Dataset格式\n",
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_val = Dataset.from_pandas(df_val)\n",
    "\n",
    "# 查看数据集信息\n",
    "print(f\"训练集样本数: {len(ds_train)}\")\n",
    "print(f\"验证集样本数: {len(ds_val)}\")\n",
    "print(\"\\n数据集前3个样本:\")\n",
    "ds_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-header",
   "metadata": {},
   "source": [
    "## 3. 加载Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tokenizer-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小: 151643\n",
      "最大长度: 16384\n",
      "PAD token: <｜end▁of▁sentence｜>\n",
      "EOS token: <｜end▁of▁sentence｜>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载tokenizer\n",
    "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    use_fast=False, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 查看tokenizer信息\n",
    "print(f\"词汇表大小: {tokenizer.vocab_size}\")\n",
    "print(f\"最大长度: {tokenizer.model_max_length}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess-header",
   "metadata": {},
   "source": [
    "## 4. 数据预处理\n",
    "\n",
    "将原始数据转换为模型可接受的格式：\n",
    "- 使用对话模板格式\n",
    "- 设置最大长度为384\n",
    "- 只对回答部分计算损失（instruction部分设为-100）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preprocess-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理训练集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9198/9198 [00:07<00:00, 1226.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集处理完成！\n",
      "\n",
      "开始处理验证集...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1500/1500 [00:01<00:00, 1413.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "验证集处理完成！\n",
      "\n",
      "处理后的第一个样本解码结果:\n",
      "<|im_start|>system\n",
      "你是PDTB文本关系分析助手<|im_end|>\n",
      "<|im_start|>user\n",
      "他的有没有什么不足之处？我觉得他可以就是加一些他自己的感受，因为他如果光只说那些一系列的动作，就感觉很空白，没有什么情感在里面。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "扩展\n",
      "原因：前半句话提出问题，询问他的不足之处，后半句话则具体回答了我认为的他的不足之处，所以属于扩展关系。<｜end▁of▁sentence｜>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 最大序列长度\n",
    "MAX_LENGTH = 384\n",
    "\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    数据处理函数\n",
    "    \n",
    "    将数据转换为对话格式：\n",
    "    <|im_start|>system\n",
    "    你是PDTB文本关系分析助手<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {用户输入}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    {模型回答}<|im_end|>\n",
    "    \"\"\"\n",
    "    # 构建指令部分（system + user）\n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\n你是PDTB文本关系分析助手<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{example.get('content', '') + example.get('input', '')}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    # 构建回答部分\n",
    "    response = tokenizer(\n",
    "        f\"{example.get('summary', '')}\", \n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    # 拼接input_ids和attention_mask\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    \n",
    "    # 构建labels：指令部分设为-100（不计算损失），只对回答部分计算损失\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "\n",
    "    # 截断到最大长度\n",
    "    input_ids = input_ids[:MAX_LENGTH]\n",
    "    attention_mask = attention_mask[:MAX_LENGTH]\n",
    "    labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids, \n",
    "        \"attention_mask\": attention_mask, \n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# 处理训练集和验证集\n",
    "print(\"开始处理训练集...\")\n",
    "tokenized_train = ds_train.map(process_func, remove_columns=ds_train.column_names)\n",
    "print(\"训练集处理完成！\")\n",
    "\n",
    "print(\"\\n开始处理验证集...\")\n",
    "tokenized_val = ds_val.map(process_func, remove_columns=ds_val.column_names)\n",
    "print(\"验证集处理完成！\")\n",
    "\n",
    "# 查看处理后的第一个样本\n",
    "print(\"\\n处理后的第一个样本解码结果:\")\n",
    "print(tokenizer.decode(tokenized_train[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-load-header",
   "metadata": {},
   "source": [
    "## 5. 加载基础模型\n",
    "\n",
    "### 关键配置说明\n",
    "- `ms_dtype=mindspore.bfloat16`: 使用bfloat16数据类型，在Ascend NPU上更稳定\n",
    "- `device_map=0`: 指定设备映射\n",
    "- `enable_input_require_grads()`: 开启输入梯度计算（LoRA训练必需）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "model-load-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型，请稍候...\n",
      "[MS_ALLOC_CONF]Runtime config:  enable_vmm:True  vmm_align_size:2MB\n",
      "模型加载完成！\n",
      "模型参数量: 1,777,088,000\n"
     ]
    }
   ],
   "source": [
    "# 加载基础模型\n",
    "print(\"正在加载模型，请稍候...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    ms_dtype=mindspore.bfloat16,  # 使用bfloat16数据类型\n",
    "    device_map=0  # 指定设备\n",
    ")\n",
    "\n",
    "# 开启梯度计算（LoRA训练必需）\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "print(\"模型加载完成！\")\n",
    "print(f\"模型参数量: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lora-config-header",
   "metadata": {},
   "source": [
    "## 6. 配置LoRA\n",
    "\n",
    "### LoRA参数说明\n",
    "- `task_type`: 任务类型（CAUSAL_LM表示因果语言模型）\n",
    "- `target_modules`: 要应用LoRA的模块（注意力层和FFN层）\n",
    "- `r=8`: LoRA秩，控制参数量\n",
    "- `lora_alpha=32`: LoRA缩放因子，通常设为r的4倍\n",
    "- `lora_dropout=0.1`: Dropout率\n",
    "- `inference_mode=False`: 训练模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lora-config-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,232,384 || all params: 1,786,320,384 || trainable%: 0.5168\n"
     ]
    }
   ],
   "source": [
    "# 配置LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # 注意力层\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"       # FFN层\n",
    "    ],\n",
    "    r=8,                    # LoRA秩\n",
    "    lora_alpha=32,          # LoRA缩放因子\n",
    "    lora_dropout=0.1,       # Dropout率\n",
    "    inference_mode=False    # 训练模式\n",
    ")\n",
    "\n",
    "# 应用LoRA到模型\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 打印可训练参数信息\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-args-header",
   "metadata": {},
   "source": [
    "## 7. 配置训练参数\n",
    "\n",
    "### 训练参数说明\n",
    "- `output_dir`: 输出目录\n",
    "- `per_device_train_batch_size=4`: 每个设备的batch size\n",
    "- `gradient_accumulation_steps=5`: 梯度累积步数（有效batch size = 4 × 5 = 20）\n",
    "- `num_train_epochs=3`: 训练轮数\n",
    "- `learning_rate=3e-5`: 学习率\n",
    "- `logging_steps=10`: 每10步记录一次日志\n",
    "- `save_steps=100`: 每100步保存一次checkpoint\n",
    "\n",
    "### ⚠️ 注意\n",
    "- **不要添加 `fp16=True`**：会导致NPU兼容性错误\n",
    "- **不要添加 `gradient_checkpointing=True`**：可能导致兼容性问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "training-args-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练参数配置完成！\n",
      "有效batch size: 20\n",
      "总训练步数: 1377\n"
     ]
    }
   ],
   "source": [
    "# 定义训练参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output\",                    # 输出目录\n",
    "    per_device_train_batch_size=4,            # batch size\n",
    "    gradient_accumulation_steps=5,            # 梯度累积步数\n",
    "    logging_steps=10,                         # 日志记录间隔\n",
    "    num_train_epochs=3,                       # 训练轮数\n",
    "    save_steps=100,                           # checkpoint保存间隔\n",
    "    learning_rate=3e-5,                       # 学习率\n",
    "    save_on_each_node=True,                   # 在每个节点上保存\n",
    "    # 注意：不要添加fp16=True或gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "print(\"训练参数配置完成！\")\n",
    "print(f\"有效batch size: {args.per_device_train_batch_size * args.gradient_accumulation_steps}\")\n",
    "print(f\"总训练步数: {len(tokenized_train) // (args.per_device_train_batch_size * args.gradient_accumulation_steps) * args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainer-header",
   "metadata": {},
   "source": [
    "## 8. 创建Trainer并开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "trainer-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.19.90, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer创建成功！\n"
     ]
    }
   ],
   "source": [
    "# 创建Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    "\n",
    "print(\"Trainer创建成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "training-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== 开始训练 ==========\n",
      "预计训练时间: 约1.5-2小时（基于之前的训练经验）\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] DEVICE(42083,ffff87443640,python):2025-11-22-15:48:21.604.645 [mindspore/ccsrc/plugin/res_manager/ascend/hal_manager/ascend_err_manager.cc:159] TaskExceptionCallback] Run Task failed, task_id: 1544, stream_id: 2, tid: 42083, device_id: 0, retcode: 507034 (Return error code unknown, ret code: 507034)\n",
      "[ERROR] DEVICE(42083,ffff87443640,python):2025-11-22-15:48:21.619.588 [mindspore/ccsrc/plugin/res_manager/ascend/stream_manager/ascend_stream_manager.cc:288] SyncStream] Call runtime aclrtSynchronizeStreamWithTimeout error.Please do the following three things to confirm whether it is caused by the execution failure of a certain operator.\n",
      "    1.Set mindspore.runtime.launch_blocking() at the beginning of your python script.\n",
      "    2.Run again your python script.\n",
      "    3.Grep 'Sync run failed' in your logs, it always stays at the end of your logs.\n",
      "Now you will get the certain failed op detailed infos.\n",
      "[ERROR] DEVICE(42083,ffff87443640,python):2025-11-22-15:48:21.619.651 [mindspore/ccsrc/plugin/res_manager/ascend/ascend_device_address/ascend_device_address.cc:416] SyncStream] Sync default stream failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Call aclnnArange failed, detail:EZ9903: [PID: 42083] 2025-11-22-15:48:21.620.860 rtKernelLaunchWithFlagV2 failed: 507034[THREAD:42887]\n        Solution: In this scenario, collect the plog when the fault occurs and locate the fault based on the plog.\n        TraceBack (most recent call last):\n        The error from device(chipId:2, dieId:0), serial number is 42, there is an exception of aivec error, core id is 1, error code = 0, dump info: pc start: 0x124041096000, current: 0x124041097260, vec error info: 0x400000010, mte error info: 0x58030100e5, ifu error info: 0x2124042800100, ccu error info: 0x501ea3605e800044, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd000288, para base: 0x124100582000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:303][THREAD:42083]\n        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x30100e5, fixp_error1 info: 0x58, fsmId:1, tslot:6, thread:0, ctxid:0, blk:13, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:322][THREAD:42083]\n        The error from device(chipId:2, dieId:0), serial number is 42, there is an exception of aivec error, core id is 2, error code = 0, dump info: pc start: 0x124041096000, current: 0x124041097260, vec error info: 0x400000010, mte error info: 0x58030100e5, ifu error info: 0x2124042800100, ccu error info: 0xcfd85c7b5e800044, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd000288, para base: 0x124100582000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:303][THREAD:42083]\n        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x30100e5, fixp_error1 info: 0x58, fsmId:1, tslot:6, thread:0, ctxid:0, blk:14, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:322][THREAD:42083]\n        The error from device(chipId:2, dieId:0), serial number is 42, there is an exception of aivec error, core id is 3, error code = 0, dump info: pc start: 0x124041096000, current: 0x124041097260, vec error info: 0x400000010, mte error info: 0x58030100e5, ifu error info: 0x2124042800100, ccu error info: 0xb19b0a1d5e800044, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd000288, para base: 0x124100582000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:303][THREAD:42083]\n        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x30100e5, fixp_error1 info: 0x58, fsmId:1, tslot:6, thread:0, ctxid:0, blk:15, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:322][THREAD:42083]\n        Kernel task happen error, retCode=0x30, [vector core timeout].[FUNC:PreCheckTaskErr][FILE:davinci_kernel_task.cc][LINE:1539][THREAD:42083]\n        AIV Kernel happen error, retCode=0x30.[FUNC:GetError][FILE:stream.cc][LINE:1183][THREAD:42083]\n        [AIC_INFO] after execute:args print end[FUNC:GetError][FILE:stream.cc][LINE:1183][THREAD:42083]\n        Aicore kernel execute failed, device_id=0, stream_id=2, report_stream_id=2, task_id=1544, flip_num=0, fault kernel_name=Range_a32d70ed63082227463347a44bd5a08e_high_performance__kernel0, fault kernel info ext=Range_a32d70ed63082227463347a44bd5a08e_high_performance__kernel0, program id=6, hash=814330933327202436.[FUNC:GetError][FILE:stream.cc][LINE:1183][THREAD:42083]\n        rtStreamSynchronizeWithTimeout execute failed, reason=[vector core timeout][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53][THREAD:42083]\n        synchronize stream failed, runtime result = 507034[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161][THREAD:42083]\n        Failed to submit kernel task, retCode=0x715005d.[FUNC:LaunchKernelSubmit][FILE:context.cc][LINE:1206][THREAD:42887]\n        kernel launch submit failed.[FUNC:LaunchKernel][FILE:context.cc][LINE:1329][THREAD:42887]\n        rtKernelLaunchWithFlagV2 execute failed, reason=[vector core timeout][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53][THREAD:42887]\n        rtKernelLaunchWithFlagV2 failed: 507034[THREAD:42887]\n        OpExecCache run fail.[THREAD:42887]\n        #### KernelLaunch failed: /usr/local/Ascend/ascend-toolkit/8.2.RC1/opp/built-in/op_impl/ai_core/tbe//kernel/ascend910b/range/Range_a32d70ed63082227463347a44bd5a08e_high_performance.o[THREAD:42887]\n        Kernel Run failed. opType: 8, Range[THREAD:42887]\n        launch failed for Range, errno:361001.[THREAD:42887]\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ops/kernel/ascend/pyboost/customize/arange.cc:39 operator()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m预计训练时间: 约1.5-2小时（基于之前的训练经验）\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/transformers/trainer.py:2238\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2239\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/transformers/trainer.py:2582\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2575\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2576\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2579\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2580\u001b[0m )\n\u001b[1;32m   2581\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2582\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2585\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2586\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2587\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2588\u001b[0m ):\n\u001b[1;32m   2589\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2590\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindnlp/transformers/trainer.py:94\u001b[0m, in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_fn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;241m=\u001b[39m autograd\u001b[38;5;241m.\u001b[39mvalue_and_grad(forward_fn, model\u001b[38;5;241m.\u001b[39mtrainable_params(), has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 94\u001b[0m loss_scaled, (loss_true,) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss_true\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/autograd/function.py:25\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.new_grad_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnew_grad_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 25\u001b[0m     values, grads \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param, grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params_or_argnums, grads):\n\u001b[1;32m     27\u001b[0m         grad \u001b[38;5;241m=\u001b[39m mindtorch\u001b[38;5;241m.\u001b[39mtensor(grad, device\u001b[38;5;241m=\u001b[39mparam\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindspore/ops/composite/base.py:638\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.after_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mafter_grad\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindspore/common/api.py:187\u001b[0m, in \u001b[0;36m_wrap_func.<locals>.wrapper\u001b[0;34m(*arg, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39marg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 187\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_python_data(results)\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindspore/ops/composite/base.py:610\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.after_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@_wrap_func\u001b[39m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mafter_grad\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 610\u001b[0m     run_args, res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pynative_forward_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_aux:\n\u001b[1;32m    612\u001b[0m         out \u001b[38;5;241m=\u001b[39m _pynative_executor\u001b[38;5;241m.\u001b[39mgrad_aux(fn, grad_, weights, grad_position, \u001b[38;5;241m*\u001b[39mrun_args)\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindspore/ops/composite/base.py:671\u001b[0m, in \u001b[0;36m_Grad._pynative_forward_run\u001b[0;34m(self, fn, grad, weights, *args, **kwargs)\u001b[0m\n\u001b[1;32m    669\u001b[0m _pynative_executor\u001b[38;5;241m.\u001b[39mset_grad_flag(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    670\u001b[0m _pynative_executor\u001b[38;5;241m.\u001b[39mnew_graph(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 671\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    672\u001b[0m _pynative_executor\u001b[38;5;241m.\u001b[39mend_graph(fn, outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    673\u001b[0m run_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindspore/ops/composite/base.py:578\u001b[0m, in \u001b[0;36m_Grad.__call__.<locals>.aux_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21maux_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 578\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen has_aux is True, origin fn requires more than one outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindnlp/transformers/trainer.py:42\u001b[0m, in \u001b[0;36mtraining_step.<locals>.forward_fn\u001b[0;34m(inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_fn\u001b[39m(inputs, num_items_in_batch):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m---> 42\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     45\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/transformers/trainer.py:3884\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3882\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3883\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m-> 3884\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3885\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3886\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/nn/modules/module.py:826\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/nn/modules/module.py:840\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m--> 840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    843\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/peft/peft_model.py:1923\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1922\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1931\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1934\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/nn/modules/module.py:826\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/nn/modules/module.py:840\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m--> 840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    843\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:308\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/transformers/utils/generic.py:959\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 959\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    961\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:450\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    432\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/nn/modules/module.py:826\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/nn/modules/module.py:840\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m--> 840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    843\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/transformers/utils/generic.py:1083\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m                 module\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m make_capture_wrapper(module, original_forward, key, specs\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   1081\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[0;32m-> 1083\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[38;5;66;03m# Restore original forward methods\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module, original_forward \u001b[38;5;129;01min\u001b[39;00m monkey_patched_layers:\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:379\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 379\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    392\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    393\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    394\u001b[0m )\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/nn/modules/module.py:826\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/nn/modules/module.py:840\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m--> 840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    843\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:229\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    219\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    227\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    228\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 229\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    232\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    233\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    240\u001b[0m     )\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/nn/modules/module.py:826\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/nn/modules/module.py:840\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m--> 840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    843\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:199\u001b[0m, in \u001b[0;36mQwen2RMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    197\u001b[0m variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    198\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindtorch/_tensor.py:2281\u001b[0m, in \u001b[0;36mTensorPlaceHolder.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2279\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39m_move_to(device, non_blocking)\n\u001b[1;32m   2280\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, mindspore\u001b[38;5;241m.\u001b[39mcommon\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mType):\n\u001b[0;32m-> 2281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m \u001b[38;5;241m==\u001b[39m arg:\n\u001b[1;32m   2282\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2283\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/work/lcl/lib/python3.10/site-packages/mindspore/common/tensor.py:581\u001b[0m, in \u001b[0;36mTensor.dtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdtype\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    Return the dtype of the tensor (:class:`mindspore.dtype`).\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03m        Float32\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Call aclnnArange failed, detail:EZ9903: [PID: 42083] 2025-11-22-15:48:21.620.860 rtKernelLaunchWithFlagV2 failed: 507034[THREAD:42887]\n        Solution: In this scenario, collect the plog when the fault occurs and locate the fault based on the plog.\n        TraceBack (most recent call last):\n        The error from device(chipId:2, dieId:0), serial number is 42, there is an exception of aivec error, core id is 1, error code = 0, dump info: pc start: 0x124041096000, current: 0x124041097260, vec error info: 0x400000010, mte error info: 0x58030100e5, ifu error info: 0x2124042800100, ccu error info: 0x501ea3605e800044, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd000288, para base: 0x124100582000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:303][THREAD:42083]\n        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x30100e5, fixp_error1 info: 0x58, fsmId:1, tslot:6, thread:0, ctxid:0, blk:13, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:322][THREAD:42083]\n        The error from device(chipId:2, dieId:0), serial number is 42, there is an exception of aivec error, core id is 2, error code = 0, dump info: pc start: 0x124041096000, current: 0x124041097260, vec error info: 0x400000010, mte error info: 0x58030100e5, ifu error info: 0x2124042800100, ccu error info: 0xcfd85c7b5e800044, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd000288, para base: 0x124100582000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:303][THREAD:42083]\n        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x30100e5, fixp_error1 info: 0x58, fsmId:1, tslot:6, thread:0, ctxid:0, blk:14, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:322][THREAD:42083]\n        The error from device(chipId:2, dieId:0), serial number is 42, there is an exception of aivec error, core id is 3, error code = 0, dump info: pc start: 0x124041096000, current: 0x124041097260, vec error info: 0x400000010, mte error info: 0x58030100e5, ifu error info: 0x2124042800100, ccu error info: 0xb19b0a1d5e800044, cube error info: 0, biu error info: 0, aic error mask: 0x6500020bd000288, para base: 0x124100582000.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:303][THREAD:42083]\n        The extend info: errcode:(0, 0, 0) errorStr: timeout or trap error. fixp_error0 info: 0x30100e5, fixp_error1 info: 0x58, fsmId:1, tslot:6, thread:0, ctxid:0, blk:15, sublk:0, subErrType:4.[FUNC:ProcessStarsCoreErrorInfo][FILE:device_error_core_proc.cc][LINE:322][THREAD:42083]\n        Kernel task happen error, retCode=0x30, [vector core timeout].[FUNC:PreCheckTaskErr][FILE:davinci_kernel_task.cc][LINE:1539][THREAD:42083]\n        AIV Kernel happen error, retCode=0x30.[FUNC:GetError][FILE:stream.cc][LINE:1183][THREAD:42083]\n        [AIC_INFO] after execute:args print end[FUNC:GetError][FILE:stream.cc][LINE:1183][THREAD:42083]\n        Aicore kernel execute failed, device_id=0, stream_id=2, report_stream_id=2, task_id=1544, flip_num=0, fault kernel_name=Range_a32d70ed63082227463347a44bd5a08e_high_performance__kernel0, fault kernel info ext=Range_a32d70ed63082227463347a44bd5a08e_high_performance__kernel0, program id=6, hash=814330933327202436.[FUNC:GetError][FILE:stream.cc][LINE:1183][THREAD:42083]\n        rtStreamSynchronizeWithTimeout execute failed, reason=[vector core timeout][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53][THREAD:42083]\n        synchronize stream failed, runtime result = 507034[FUNC:ReportCallError][FILE:log_inner.cpp][LINE:161][THREAD:42083]\n        Failed to submit kernel task, retCode=0x715005d.[FUNC:LaunchKernelSubmit][FILE:context.cc][LINE:1206][THREAD:42887]\n        kernel launch submit failed.[FUNC:LaunchKernel][FILE:context.cc][LINE:1329][THREAD:42887]\n        rtKernelLaunchWithFlagV2 execute failed, reason=[vector core timeout][FUNC:FuncErrorReason][FILE:error_message_manage.cc][LINE:53][THREAD:42887]\n        rtKernelLaunchWithFlagV2 failed: 507034[THREAD:42887]\n        OpExecCache run fail.[THREAD:42887]\n        #### KernelLaunch failed: /usr/local/Ascend/ascend-toolkit/8.2.RC1/opp/built-in/op_impl/ai_core/tbe//kernel/ascend910b/range/Range_a32d70ed63082227463347a44bd5a08e_high_performance.o[THREAD:42887]\n        Kernel Run failed. opType: 8, Range[THREAD:42887]\n        launch failed for Range, errno:361001.[THREAD:42887]\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/ops/kernel/ascend/pyboost/customize/arange.cc:39 operator()\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "print(\"========== 开始训练 ==========\")\n",
    "print(\"预计训练时间: 约1.5-2小时（基于之前的训练经验）\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 9. 训练总结\n",
    "\n",
    "训练完成后：\n",
    "- LoRA权重保存在 `./output/checkpoint-xxxx/` 目录中\n",
    "- 主要文件包括：\n",
    "  - `adapter_model.safetensors`: LoRA适配器权重\n",
    "  - `adapter_config.json`: LoRA配置\n",
    "  - `trainer_state.json`: 训练状态\n",
    "\n",
    "### 下一步\n",
    "使用 `merge.ipynb` 脚本将LoRA权重与基础模型合并，或直接使用checkpoint进行推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练结果文件\n",
    "import os\n",
    "print(\"训练输出目录内容:\")\n",
    "if os.path.exists('./output'):\n",
    "    for item in os.listdir('./output'):\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"输出目录尚未创建\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lcl)",
   "language": "python",
   "name": "lcl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
