{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# LoRA权重合并脚本\n",
    "\n",
    "## 功能说明\n",
    "\n",
    "本脚本用于将训练好的LoRA适配器权重与基础模型合并，生成完整的微调模型。\n",
    "\n",
    "### 主要步骤\n",
    "1. 加载基础模型和tokenizer\n",
    "2. 加载LoRA适配器权重\n",
    "3. 合并权重\n",
    "4. 保存合并后的完整模型\n",
    "5. 推理测试验证效果\n",
    "\n",
    "### 版本信息\n",
    "- mindnlp: 0.5.1\n",
    "- mindspore: 2.7.0\n",
    "- transformers: ~4.40-4.45（推荐）\n",
    "\n",
    "### 适用场景\n",
    "- 将LoRA权重永久合并到基础模型\n",
    "- 部署时不依赖LoRA适配器\n",
    "- 简化推理流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-header",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核心框架\n",
    "import mindnlp\n",
    "import mindspore\n",
    "from mindnlp import core\n",
    "\n",
    "# 模型相关\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# 查看版本信息\n",
    "print(f\"mindnlp版本: {mindnlp.__version__}\")\n",
    "print(f\"mindspore版本: {mindspore.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. 配置路径\n",
    "\n",
    "### 路径说明\n",
    "- `base_model_name`: 基础模型名称（从Hugging Face加载）\n",
    "- `lora_path`: 训练好的LoRA checkpoint路径\n",
    "- `merged_path`: 合并后模型的保存路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础模型名称\n",
    "base_model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "\n",
    "# 训练好的 LoRA checkpoint 路径（修改为你的实际checkpoint路径）\n",
    "# 示例：如果你的最佳checkpoint是checkpoint-1380，则设置为：\n",
    "lora_path = \"/home/ma-user/work/output/checkpoint-1380\"\n",
    "\n",
    "# 合并后模型的保存目录\n",
    "merged_path = \"/home/ma-user/work/merged_model\"\n",
    "\n",
    "print(f\"基础模型: {base_model_name}\")\n",
    "print(f\"LoRA权重路径: {lora_path}\")\n",
    "print(f\"合并后保存路径: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-base-header",
   "metadata": {},
   "source": [
    "## 3. 加载基础模型和Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-tokenizer-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载tokenizer\n",
    "print(\"正在加载tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name, \n",
    "    use_fast=False, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(\"Tokenizer加载完成！\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-base-model-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载基础模型\n",
    "print(\"正在加载基础模型，请稍候...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    ms_dtype=mindspore.bfloat16,  # 使用bfloat16数据类型\n",
    "    device_map=0  # 指定设备\n",
    ")\n",
    "\n",
    "print(\"基础模型加载完成！\")\n",
    "print(f\"模型参数量: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-lora-header",
   "metadata": {},
   "source": [
    "## 4. 加载LoRA权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-lora-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 LoRA 适配器权重\n",
    "print(\"正在加载LoRA适配器权重...\")\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "print(\"LoRA权重加载完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merge-header",
   "metadata": {},
   "source": [
    "## 5. 合并权重\n",
    "\n",
    "使用 `merge_and_unload()` 方法将LoRA权重合并到基础模型中，生成完整的微调模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并 LoRA 权重到基础模型\n",
    "print(\"正在合并权重...\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"权重合并完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 6. 保存合并后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存完整的微调模型\n",
    "print(f\"正在保存模型到 {merged_path}...\")\n",
    "model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"✅ LoRA 权重已成功合并！\")\n",
    "print(f\"✅ 合并后的模型保存在: {merged_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference-header",
   "metadata": {},
   "source": [
    "## 7. 推理测试\n",
    "\n",
    "测试合并后的模型是否正常工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将模型移至NPU设备\n",
    "print(\"正在将模型移至NPU设备...\")\n",
    "model = model.npu()\n",
    "print(\"模型已就绪！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inference-test-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试样例\n",
    "test_prompt = \"月亮又圆又亮，所以古人称之为玉盘。\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"推理测试\")\n",
    "print(\"=\"*60)\n",
    "print(f\"输入文本: {test_prompt}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# 构建对话输入\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"你是PDTB文本关系分析助手\"},\n",
    "        {\"role\": \"user\", \"content\": test_prompt}\n",
    "    ],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_tensors=\"ms\",\n",
    "    return_dict=True\n",
    ").to('cuda')\n",
    "\n",
    "# 生成配置\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 2500,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 1\n",
    "}\n",
    "\n",
    "# 生成回答\n",
    "with core.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    # 只保留生成的部分（去除输入）\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "print(\"模型输出:\")\n",
    "print(response)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "more-tests-header",
   "metadata": {},
   "source": [
    "## 8. 更多测试样例（可选）\n",
    "\n",
    "您可以在下方添加更多测试样例来验证模型效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "more-tests-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义测试函数，方便测试多个样例\n",
    "def test_model(prompt):\n",
    "    \"\"\"\n",
    "    测试模型推理\n",
    "    \n",
    "    参数:\n",
    "        prompt: 输入文本\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"输入: {prompt}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": \"你是PDTB文本关系分析助手\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_tensors=\"ms\",\n",
    "        return_dict=True\n",
    "    ).to('cuda')\n",
    "    \n",
    "    gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "    \n",
    "    with core.no_grad():\n",
    "        outputs = model.generate(**inputs, **gen_kwargs)\n",
    "        outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"输出:\")\n",
    "    print(response)\n",
    "    print(\"=\"*60)\n",
    "    print()\n",
    "\n",
    "# 测试样例列表\n",
    "test_cases = [\n",
    "    \"他的有没有什么不足之处？我觉得他可以就是加一些他自己的感受，因为他如果光只说那些一系列的动作，就感觉很空白，没有什么情感在里面。\",\n",
    "    \"星汉是什么？银河。\",\n",
    "    \"对于花来说没有人欣赏是多么的悲惨，就像我们姑娘把自己打扮得花枝招展，却没有人欣赏一样是一种不幸\"\n",
    "]\n",
    "\n",
    "# 执行测试\n",
    "for test_case in test_cases:\n",
    "    test_model(test_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 9. 总结\n",
    "\n",
    "### 完成的工作\n",
    "1. ✅ 加载基础模型和LoRA适配器\n",
    "2. ✅ 合并LoRA权重到基础模型\n",
    "3. ✅ 保存完整的微调模型\n",
    "4. ✅ 推理测试验证效果\n",
    "\n",
    "### 模型使用\n",
    "合并后的模型可以直接用于推理，无需再加载LoRA适配器：\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 直接加载合并后的模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(merged_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(merged_path)\n",
    "```\n",
    "\n",
    "### 下一步\n",
    "- 可以将合并后的模型部署到生产环境\n",
    "- 或者继续在验证集上评估模型性能"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
