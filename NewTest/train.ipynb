{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-cell",
   "metadata": {},
   "source": [
    "# 中文篇章级句间关系分类 - LoRA微调实验\n",
    "\n",
    "## 实验说明\n",
    "\n",
    "本实验使用 **MindSpore + mindnlp 0.5.1 + LoRA** 在 **DeepSeek-R1-Distill-Qwen-1.5B** 模型上进行微调。\n",
    "\n",
    "### 任务目标\n",
    "- **输入**：一个句子或对话片段\n",
    "- **输出**：该句子所属的PDTB篇章关系分类（扩展/因果/比较/并列/其他）以及分类原因\n",
    "\n",
    "### 版本信息\n",
    "- mindnlp: 0.5.1\n",
    "- mindspore: 2.7.0\n",
    "- transformers: ~4.40-4.45（推荐）\n",
    "- 数据类型: bfloat16（在Ascend NPU上更稳定）\n",
    "\n",
    "### ⚠️ 重要注意事项\n",
    "1. **不要使用 `fp16=True`**：会导致 `mindtorch.npu.amp` 错误\n",
    "2. **不要使用 `gradient_checkpointing=True`**：可能导致兼容性问题\n",
    "3. **使用 `bfloat16` 数据类型**：在Ascend NPU上表现更稳定\n",
    "4. **数据路径**：确保数据已上传至 `/home/ma-user/work/data/`\n",
    "\n",
    "### 训练环境\n",
    "- 镜像：mindspore_2_7-vllm-mindspeed-cann8_2alpha2_ubuntu22\n",
    "- 实例规格：Ascend: 1*ascend-snt9b1 | ARM: 24核 192GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-cell-header",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核心框架\n",
    "import mindnlp\n",
    "import mindspore\n",
    "from mindnlp import core\n",
    "\n",
    "# 数据处理\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 模型和训练相关\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    DataCollatorForSeq2Seq, \n",
    "    TrainingArguments, \n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# LoRA相关\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "# 查看版本信息\n",
    "print(f\"mindnlp版本: {mindnlp.__version__}\")\n",
    "print(f\"mindspore版本: {mindspore.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-load-header",
   "metadata": {},
   "source": [
    "## 2. 加载数据集\n",
    "\n",
    "数据集格式：\n",
    "```json\n",
    "{\n",
    "    \"content\": \"输入的句子或对话内容\",\n",
    "    \"summary\": \"关系分类结果\\n原因：详细解释\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-load-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据路径（华为云ModelArts路径）\n",
    "train_path = \"/home/ma-user/work/data/train.json\"\n",
    "val_path = \"/home/ma-user/work/data/val.json\"\n",
    "\n",
    "# 读取数据\n",
    "df_train = pd.read_json(train_path)\n",
    "df_val = pd.read_json(val_path)\n",
    "\n",
    "# 转换为Dataset格式\n",
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_val = Dataset.from_pandas(df_val)\n",
    "\n",
    "# 查看数据集信息\n",
    "print(f\"训练集样本数: {len(ds_train)}\")\n",
    "print(f\"验证集样本数: {len(ds_val)}\")\n",
    "print(\"\\n数据集前3个样本:\")\n",
    "ds_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-header",
   "metadata": {},
   "source": [
    "## 3. 加载Tokenizer\n",
    "\n",
    "使用 DeepSeek-R1-Distill-Qwen-1.5B 的分词器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载tokenizer\n",
    "model_name = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    use_fast=False, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 查看tokenizer信息\n",
    "print(f\"词汇表大小: {tokenizer.vocab_size}\")\n",
    "print(f\"最大长度: {tokenizer.model_max_length}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess-header",
   "metadata": {},
   "source": [
    "## 4. 数据预处理\n",
    "\n",
    "将原始数据转换为模型可接受的格式：\n",
    "- 使用对话模板格式\n",
    "- 设置最大长度为384\n",
    "- 只对回答部分计算损失（instruction部分设为-100）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocess-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大序列长度\n",
    "MAX_LENGTH = 384\n",
    "\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    数据处理函数\n",
    "    \n",
    "    将数据转换为对话格式：\n",
    "    <|im_start|>system\n",
    "    你是PDTB文本关系分析助手<|im_end|>\n",
    "    <|im_start|>user\n",
    "    {用户输入}<|im_end|>\n",
    "    <|im_start|>assistant\n",
    "    {模型回答}<|im_end|>\n",
    "    \"\"\"\n",
    "    # 构建指令部分（system + user）\n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\n你是PDTB文本关系分析助手<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{example.get('content', '') + example.get('input', '')}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    # 构建回答部分\n",
    "    response = tokenizer(\n",
    "        f\"{example.get('summary', '')}\", \n",
    "        add_special_tokens=False\n",
    "    )\n",
    "\n",
    "    # 拼接input_ids和attention_mask\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    \n",
    "    # 构建labels：指令部分设为-100（不计算损失），只对回答部分计算损失\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "\n",
    "    # 截断到最大长度\n",
    "    input_ids = input_ids[:MAX_LENGTH]\n",
    "    attention_mask = attention_mask[:MAX_LENGTH]\n",
    "    labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids, \n",
    "        \"attention_mask\": attention_mask, \n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# 处理训练集和验证集\n",
    "print(\"开始处理训练集...\")\n",
    "tokenized_train = ds_train.map(process_func, remove_columns=ds_train.column_names)\n",
    "print(\"训练集处理完成！\")\n",
    "\n",
    "print(\"\\n开始处理验证集...\")\n",
    "tokenized_val = ds_val.map(process_func, remove_columns=ds_val.column_names)\n",
    "print(\"验证集处理完成！\")\n",
    "\n",
    "# 查看处理后的第一个样本\n",
    "print(\"\\n处理后的第一个样本解码结果:\")\n",
    "print(tokenizer.decode(tokenized_train[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-load-header",
   "metadata": {},
   "source": [
    "## 5. 加载基础模型\n",
    "\n",
    "### 关键配置说明\n",
    "- `ms_dtype=mindspore.bfloat16`: 使用bfloat16数据类型，在Ascend NPU上更稳定\n",
    "- `device_map=0`: 指定设备映射\n",
    "- `enable_input_require_grads()`: 开启输入梯度计算（LoRA训练必需）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-load-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载基础模型\n",
    "print(\"正在加载模型，请稍候...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    ms_dtype=mindspore.bfloat16,  # 使用bfloat16数据类型\n",
    "    device_map=0  # 指定设备\n",
    ")\n",
    "\n",
    "# 开启梯度计算（LoRA训练必需）\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "print(\"模型加载完成！\")\n",
    "print(f\"模型参数量: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lora-config-header",
   "metadata": {},
   "source": [
    "## 6. 配置LoRA\n",
    "\n",
    "### LoRA参数说明\n",
    "- `task_type`: 任务类型（CAUSAL_LM表示因果语言模型）\n",
    "- `target_modules`: 要应用LoRA的模块（注意力层和FFN层）\n",
    "- `r=8`: LoRA秩，控制参数量\n",
    "- `lora_alpha=32`: LoRA缩放因子，通常设为r的4倍\n",
    "- `lora_dropout=0.1`: Dropout率\n",
    "- `inference_mode=False`: 训练模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lora-config-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # 注意力层\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"       # FFN层\n",
    "    ],\n",
    "    r=8,                    # LoRA秩\n",
    "    lora_alpha=32,          # LoRA缩放因子\n",
    "    lora_dropout=0.1,       # Dropout率\n",
    "    inference_mode=False    # 训练模式\n",
    ")\n",
    "\n",
    "# 应用LoRA到模型\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 打印可训练参数信息\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-args-header",
   "metadata": {},
   "source": [
    "## 7. 配置训练参数\n",
    "\n",
    "### 训练参数说明\n",
    "- `output_dir`: 输出目录\n",
    "- `per_device_train_batch_size=4`: 每个设备的batch size\n",
    "- `gradient_accumulation_steps=5`: 梯度累积步数（有效batch size = 4 × 5 = 20）\n",
    "- `num_train_epochs=3`: 训练轮数\n",
    "- `learning_rate=3e-5`: 学习率\n",
    "- `logging_steps=10`: 每10步记录一次日志\n",
    "- `save_steps=100`: 每100步保存一次checkpoint\n",
    "\n",
    "### ⚠️ 注意\n",
    "- **不要添加 `fp16=True`**：会导致NPU兼容性错误\n",
    "- **不要添加 `gradient_checkpointing=True`**：可能导致兼容性问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-args-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output\",                    # 输出目录\n",
    "    per_device_train_batch_size=4,            # batch size\n",
    "    gradient_accumulation_steps=5,            # 梯度累积步数\n",
    "    logging_steps=10,                         # 日志记录间隔\n",
    "    num_train_epochs=3,                       # 训练轮数\n",
    "    save_steps=100,                           # checkpoint保存间隔\n",
    "    learning_rate=3e-5,                       # 学习率\n",
    "    save_on_each_node=True,                   # 在每个节点上保存\n",
    "    # 注意：不要添加fp16=True或gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "print(\"训练参数配置完成！\")\n",
    "print(f\"有效batch size: {args.per_device_train_batch_size * args.gradient_accumulation_steps}\")\n",
    "print(f\"总训练步数: {len(tokenized_train) // (args.per_device_train_batch_size * args.gradient_accumulation_steps) * args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainer-header",
   "metadata": {},
   "source": [
    "## 8. 创建Trainer并开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trainer-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    "\n",
    "print(\"Trainer创建成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "print(\"========== 开始训练 ==========\")\n",
    "print(\"预计训练时间: 约1.5-2小时（基于之前的训练经验）\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## 9. 训练总结\n",
    "\n",
    "训练完成后：\n",
    "- LoRA权重保存在 `./output/checkpoint-xxxx/` 目录中\n",
    "- 主要文件包括：\n",
    "  - `adapter_model.safetensors`: LoRA适配器权重\n",
    "  - `adapter_config.json`: LoRA配置\n",
    "  - `trainer_state.json`: 训练状态\n",
    "\n",
    "### 下一步\n",
    "使用 `merge.ipynb` 脚本将LoRA权重与基础模型合并，或直接使用checkpoint进行推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练结果文件\n",
    "import os\n",
    "print(\"训练输出目录内容:\")\n",
    "if os.path.exists('./output'):\n",
    "    for item in os.listdir('./output'):\n",
    "        print(f\"  - {item}\")\n",
    "else:\n",
    "    print(\"输出目录尚未创建\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
