# 面向课堂场景的大模型定制化微调——逻辑问答交互与授课效果关联性研究

各位老师、同学们，大家好！

我是刘承龙，今天特别荣幸能跟大家分享我做的项目——基于昇思MindSpore的课堂问答逻辑关系分析系统。接下来我就用10分钟左右，从自己做项目的实际经历出发，讲讲背景、方案，尤其是我写代码、调模型的细节。

## 目录
项目共分为四个部分，分别是：
1. 项目背景
2. 项目简介
3. 技术方案
4. 使用方法

### 一、项目背景
在课堂分析领域，有个专门的任务是分析”教师与学生间的互动对授课效果的影响”。具体而言，就是要分析教师在课堂上的提问、回答、互动，是否符合逻辑、是否能真正地帮助学生理解知识。

当我们能够准确分析教师与学生间的互动逻辑关系时，就能更准确地判断提问是否符合逻辑、是否能帮助学生理解知识。从而更有效地优化提问，提升授课效果，掌握学生跟进课堂程度，促进教育发展。

但是，根据我查阅资料后发现，目前的课堂分析领域一般分为两个方向：
1. 基于传统的特征工程方法，手动提取问答中的逻辑关系特征，然后用分类模型判断。这种方法的优势是简单、可解释性好，但缺点是需要人工提取特征，使劲成本和人工成本非常高。
2. 基于深度学习模型，他通过 Transformer 等经典的深度学习模型直接对问答文本进行分类。这种方法的优势是快速且自动化，并能较容易地迁移到新的任务或领域。但缺点是模型属于黑盒模型，输出解释性差。

### 二、项目简介
因此，我决定基于昇思 MindSpore 框架，实现一个课堂问答逻辑关系分析系统，使之在自动化高效分析地同时，还能兼顾到一定的可解释性。

简单来说，我将使用昇思 MindSpore 框架，基于 LoRA 微调技术，对预训练的 NLP 模型进行定制化微调。本项目提供了模型训练代码和相应的端到端封装代码，用户可以根据自己的需求进行定制化配置。

### 三、技术方案
下面我们对技术方案进行详细讲解。

首先是“基础模型”。我选择的基础模型是 DeepSeek-R1-Distil-Qwen-1.5B，它基于 Qwen 架构，由强大的 Deepseek-R1 模型蒸馏而来，参数只有 17 亿，但是性能却非常好，支持多步逻辑推理、数学推理、文本生成、代码编写等多种任务。

第二部分是“数据处理”。本项目使用的原始数据集是“哈工大中文篇章关系语料”，简称 CDTB 数据集。它包含了大约 10000 条中文问答数据，每个问答数据都有一个逻辑关系标签。大家可通过 `https://ir.hit.edu.cn/2024/1029/c19699a357757/page.htm` 进行访问和下载。

CDTB数据集是基于 PDTB 理论体系的中文篇章关系数据集。它将中文问答数据分为了6类逻辑关系：扩展、因果、比较、并列、条件和时序。其中由于条件关系和时序关系的样本数较少，本项目中将其合并为“其他”类。

为了符合本项目的任务，我对 CDTB 数据库进行了一点的处理。具体来看，CDTB 数据库中每条样本的标签中仅包含了该样本的分类结果，这注定不能提供良好的可解释性。为此，我在系统学习了中文篇章关系分类体系知识后，为其中的 2000 条样本人工添加了判断原因。而对于其他 8000 条样本，即使没有写上详细判断原因，我也为他们添加了“原因：”字样，以保证模型充分学习到本项目的任务要求。

接下来是“核心技术”。本项目基于昇思 MindSpore 框架，使用到了 MindSpore、MindNLP、Transformers 等库。其中，
- MindSpore：是一个基于图计算的深度学习框架，提供了丰富的神经网络层、损失函数、优化器等组件，同时支持动态图和静态图模式。
- MindNLP：是一个基于 MindSpore 的 NLP 工具库，提供了常用的 NLP 模型、数据集、评估指标等组件。
- Transformers：是一个基于 PyTorch 的 NLP 工具库，提供了常用的预训练模型、.tokenizer 等组件。

他们可以统一使用 `pip install mindnlp==0.5.1` 来进行安装，这条命令会自动安装好 MindNLP、Transformers 等其他所需要的库。

刚才所讲到的是本项目使用的框架技术，下面我们来具体介绍本项目的核心技术——LoRA 微调。在这里有关于 LoRA 的配置介绍

---

### 三、代码细节：我怎么一步步实现的？（重点讲我做的）
这部分是我花时间最多的地方，从搭环境到写推理代码，每一步都踩过坑，最后终于跑通了，我跟大家慢慢说。

#### （一）第一步：搭环境——我踩过版本冲突的坑
最开始我用的是华为云的ModelArts平台，选了ms2.7.1-cann8.2rc1这个镜像，因为里面自带昇腾的驱动，不用自己装。然后我建了个虚拟环境，怕依赖冲突：

我先在终端输`python3 -m venv lcl`，建了个叫lcl的环境，然后激活它——Linux下输`source /home/ma-user/work/lcl/bin/activate`，Windows要换个命令。接着装库，这里特别要注意版本：比如`pip install mindnlp==0.5.1`，`transformers==4.57.3`，因为我试过更高版本的transformers，跟MindSpore 2.7.0不兼容，跑的时候直接报错，后来调试了好几次才找到适配的版本。

另外，为了让老师能用，我还搭了网页环境：用conda建了个叫mindtrio的环境，装了Flask、torch这些，写了个requirements.txt，别人拿过去直接`pip install -r`就能用，不用再一个个找库。

#### （二）第二步：处理数据——我写了个关键的process_func函数
数据是哈工大的CDTB语料，还有我们自己标的数据，都是JSON格式。我先用pandas读进来：

```python
df_train = pd.read_json(train_path)
ds_train = Dataset.from_pandas(df_train)
```
转成Dataset格式，是因为后面用transformers处理更方便。然后最关键的是我写的process_func函数——这个函数是让模型知道“该学什么”。

我给大家解释下这个函数的逻辑：模型要同时做两件事——判断逻辑关系，还要写原因。所以我把输入分成了“指令”和“回答”两部分：指令告诉模型“你是PDTB分析助手，用户给你问答文本，你要输出结果”；回答就是模型要学的“逻辑类型+原因”。

这里有个我琢磨了好久的技巧：labels里，我把指令部分设成了-100。为啥？因为我不想让模型学“重复指令”，只让它学“怎么根据指令写回答”。如果不设-100，模型会跟着指令重复，根本出不来正确结果，当时调试的时候，第一次没设，模型输出全是“你是PDTB助手”，后来改了这个才好。

最后用`ds_train.map(process_func, batched=True)`批量处理数据，训练集9198条，验证集1500条，数据就准备好了。

#### （三）第三步：加载模型+LoRA配置——我解决了显存不够的问题
最开始我想全量微调模型，结果17亿参数的模型，显存直接爆了，根本跑不起来。后来查资料知道了LoRA，就试着用这个技术：

首先加载基础模型，我用了bfloat16精度，比float32省一半显存，精度还没什么损失：

```python
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    ms_dtype=mindspore.bfloat16,
    device_map=0
)
model = model.to('npu:0')
model.enable_input_require_grads()
```
然后配置LoRA，这里的参数是我调了好几次的：target_modules选了7个层——注意力层的q_proj、k_proj这些，还有FFN层的gate_proj，因为这些层对逻辑判断影响最大；r设16，alpha32，这样算下来，可训练参数只有197万，不到基础模型的0.1%！

我当时还特意用`model.print_trainable_parameters()`看了下，输出“trainable%: 0.1105”，特别开心——原来要训几天的模型，现在1个半小时就能跑完。

#### （四）第四步：训练模型——我盯着损失下降特别激动
训练参数是我跟团队一起定的：批量4，梯度累积5步（这样有效批量就是20，训练更稳），学习率3e-5，训3轮，每10步打一次日志。

跑的时候我就盯着屏幕看损失：刚开始是4.5，然后慢慢降到3.5、2.4，最后到1.2左右，验证集损失也跟着降，说明模型没学偏，收敛得很好。当时看到第100步的时候，损失降到1.8，我还跟队友说“有戏！”

#### （五）第五步：推理+部署——我让老师不用懂代码也能用
训练完不能光放着，得能实际用。首先我把LoRA权重和基础模型合并了，这样部署的时候不用带两个文件，直接一个模型就行：

```python
merged_model = PeftModel.from_pretrained(base_model, lora_path)
merged_model = merged_model.merge_and_unload()
```
然后写推理代码，我测试了个例子：“月亮又圆又亮，所以古人称之为玉盘。” 模型输出“因果，原因：前半句话说月亮的特征，后半句说古人的称呼，是因果关系”——完全对！当时我还拿给我们系的老师看，老师说“这个解释能看懂，有用”。

为了让老师方便用，我用Flask搭了个网页：写了个app.py，老师运行`python app.py`，打开浏览器输`http://localhost:5000`，直接在网页里输问答文本，点个按钮就能出结果。上次我们系教研活动，有老师现场试了，说比人工分析快多了。

### 四、做出来的效果：我觉得最有价值的三点
1. 能落地：模型准确率85%以上，原因生成80%准，老师真的能用，不是光做个样子。
2. 省成本：用LoRA训，昇腾192GB内存的机器就能跑，不用特别贵的硬件。
3. 全开源：代码我都放到GitHub上了（https://github.com/GuoDragon/MindSpore_CCNU_MindTrio），谁想复现、改一改用在自己学校，都能拿去。

### 五、接下来想做的：我还有三个小目标
1. 加数据增强代码：现在数据还是有点少，我想写个脚本，自动生成同义的问答文本，让模型泛化更好。
2. 压缩模型：现在模型还是有点大，我想试试MindSpore的量化工具，把体积压小，让教室的本地服务器也能跑。
3. 支持语音：以后想加个语音转文本的功能，老师直接传课堂录音，不用手动输文本，更方便。

### 六、最后：我想谢谢的人
做这个项目的时候，踩了好多坑——比如LoRA训的时候报错、显存溢出，都是一点点查资料、调参数过来的。特别感谢昇思MindSpore给的框架和算力，没有这个，LoRA根本跑不起来；也谢谢陈强、张浩宇，跟我一起标数据、调代码；最感谢各位评委老师，今天听我讲这么多细节。

我一直觉得，做AI项目不是为了炫技术，而是要解决实际问题。这个项目能帮老师省点事、让课堂互动更好，我觉得特别有意义。我的汇报就到这儿，谢谢大家！有问题的话，我很乐意跟大家交流！